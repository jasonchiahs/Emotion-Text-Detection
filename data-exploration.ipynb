{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Objective\n",
    "Aim of this project is to determine the writer emotion based on their text that they had written.\n",
    "We will use Natural Process Language to different the different types of emotions.\n",
    "\n",
    "Emotions are classified into six categories: \n",
    "1. Sadness (0)\n",
    "2. Joy (1)\n",
    "3. Love (2)\n",
    "4. Anger (3) \n",
    "5. Fear (4) \n",
    "6. Surprise (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_dataset = pd.read_csv(\"raw_data/text_emotion_Dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Raw data consist of: {origin_dataset.shape[0]} rows\")\n",
    "print(f\"Raw data consist of: {origin_dataset.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_dataset[origin_dataset.duplicated()] # Good that we don't have duplicate rows or dulicapted dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_dataset['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data wraggling/Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_dataset = origin_dataset[['text','label']].copy()\n",
    "emotion_dataset['label'] = emotion_dataset['label'].astype('category')\n",
    "emotion_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {\n",
    "    0: 'Sadness',\n",
    "    1: 'Joy',\n",
    "    2: 'Love',\n",
    "    3: 'Anger',\n",
    "    4: 'Fear',\n",
    "    5: 'Surprise'\n",
    "}\n",
    "emotion_dataset['emotion_label'] = emotion_dataset['label'].map(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_dataset['emotion_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_dataset['word_length'] = emotion_dataset['text'].apply(lambda x: len(x.split()))\n",
    "emotion_dataset[['text','word_length','emotion_label','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(emotion_dataset['emotion_label'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=emotion_dataset,x='emotion_label')\n",
    "fig, axs =  plt.subplots(3,2, figsize = (15,18))\n",
    "axs = axs.flatten()\n",
    "for i, emotion in enumerate(list(emotion_dataset['emotion_label'].unique())):\n",
    "    subplot = emotion_dataset[emotion_dataset['emotion_label'] == emotion]\n",
    "    sns.histplot(data=subplot,\n",
    "                 x = 'word_length',\n",
    "                 ax=axs[i],\n",
    "                 bins = 50)\n",
    "    axs[i].set_title(f\"Word length for {emotion}\")\n",
    "    axs[i].set_xlabel(\"Word length\")\n",
    "    axs[i].set_ylabel(\"Frequency\")\n",
    "    axs[i].legend([emotion])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing: Text proprocessing\n",
    "- lowercase\n",
    "- dealing with numbers, punctuation, and symbols\n",
    "- splitting\n",
    "- tokenizing\n",
    "- removing \"stopwords\"\n",
    "- lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_text(text):\n",
    "    text = text.str.lower()\n",
    "    text = text.str.replace(r'http\\S+', '', regex=True)\n",
    "    text = text.str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "    text = text.str.replace(r'\\s+', ' ', regex=True)\n",
    "    text = text.str.replace(r'\\d+', '', regex=True)\n",
    "    text = text.str.strip()\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(sentence):\n",
    "    stop_words  = set(stopwords.words('english'))\n",
    "    filtered_sentence = [word for word in sentence.split() if word not in stop_words]\n",
    "    return ' '.join(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_dataset['text'] = preprocessing_text(emotion_dataset['text'])\n",
    "emotion_dataset['text'] = emotion_dataset['text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_dataset.drop(columns = ['emotion_label','word_length'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "# from keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "max_words= 50000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = emotion_dataset['text']\n",
    "y = emotion_dataset['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer(num_words=input_size)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "# tokenizer.fit_on_texts(X_test)\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum lenght in the sequence set\n",
    "maxlen = max(len(tokens) for tokens in X_train_sequences)\n",
    "print(\"Maximum sequence length (maxlen):\", maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform padding on X_train and X_test sequences\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=maxlen, padding='post',)\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=maxlen, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Embedding, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=128, input_length=maxlen))\n",
    "model.add(GRU(64, return_sequences=False))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(6, activation='softmax'))  # 6 classes for the emotions\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_accuracy',  # or 'val_accuracy'\n",
    "    patience=3,          # Number of epochs with no improvement after which training will be stopped\n",
    "    restore_best_weights=True  # Restore model weights from the epoch with the best value of the monitored quantity\n",
    ")\n",
    "\n",
    "model.fit(X_train_padded, y_train, epochs=5, batch_size=32, validation_split=0.2, callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example of keras tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t  = Tokenizer()\n",
    "fit_text = \"The earth is an awesome place live\"\n",
    "t.fit_on_texts(fit_text)\n",
    "test_text = \"The earth is an great place live\"\n",
    "sequences = t.texts_to_sequences(test_text)\n",
    "\n",
    "print(\"sequences : \",sequences,'\\n')\n",
    "\n",
    "print(\"word_index : \",t.word_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Emotion-Text-Detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
